---
title: "重排模型"
description: "帮助智能体理解关联并抓取上下文"
icon: "medal"
---

## 重排器：MemMachine 的精度调谐器

想象一下，你的 MemMachine 智能体拥有一个庞大的记忆库，保存着过去的对话、事实与用户偏好。当用户提问时，MemMachine 会迅速从中挑出一叠可能相关的“书”。这一初始检索快速且范围宽广，但并不是每一本都与用户的具体问题同等相关。此时就轮到 **重排器（re-ranker）** 登场了！

### 重排器在 MemMachine 输出中的作用

重排器就像一位**高效的图书管理员**，深谙用户问题的真实意图。它会重新阅读初步检索出来的记忆片段，在原始问题的语境下逐个评估，并对它们**重新排序**，将最相关、最准确的信息推到最前。

可以这样理解：
- **初始检索：** 找出所有与 “猫” 有关的书。
- **重排器：** 当用户问 “如何给暹罗猫梳毛？” 时，把那本关于 “暹罗猫梳理技巧” 的书排在第一位，尽管其他猫咪书籍也被检索到了。

## MemMachine 支持的重排器

MemMachine 支持多种重排器，每种都有各自优势，也可以根据具体场景灵活配置。

### Cross-Encoder

Cross-Encoder 通常基于 Transformer 架构（如 BERT、RoBERTa 等），用于在一次编码中评估两段文本的相似度或相关性。

基本流程：

1. 将完整查询与候选文档/记忆片段拼接成单个输入。
2. 模型共同处理两段文本，实现词与词之间深度交互（“cross” 的含义即在于此）。
3. 输出一条评分，表示该查询与文档的相关程度。

由于能对查询与文档进行细粒度比较，Cross-Encoder 非常适合作为重排器。

### Embedder

**嵌入模型（Embedder）** 会将输入（通常为文本，亦可为图像、音频等）转换为密集向量，捕捉语义与上下文。语义相近的文本在向量空间中的距离更近。

更多信息可参阅 [嵌入模型](./embedding.mdx)。

### BM25

BM25 是经典且高效的关键词排序算法，属于词汇检索（Lexical Search）。它根据如下因素对文档进行评分：
- **词频（TF）：** 查询中的词在文档中出现的频率。
- **逆文档频率（IDF）：** 词在所有文档中出现的稀有程度，越稀有权重越高。
- **文档长度：** 对超长文档进行惩罚，防止高词频造成偏差。

最终得到的相关性得分可用于对文档排序。

**BM25** 常作为两阶段或多阶段检索流程中的第一阶段：
- **初始检索：** 当智能体收到查询时，BM25（或类似方法）会迅速从海量记忆片段中筛出包含相关关键词的候选集合，作为“候选生成”步骤。
- **重排阶段：** 随后重排器会针对这个较小的集合（例如前 50 或 100 个候选）进行深度评分与排序。

### Identity

在数学与计算中，“Identity” 操作意味着**输出与输入完全一致**，没有任何改变。若配置为 Identity 重排器，将直接返回初始检索的结果顺序，不做再排序。

在 MemMachine 中，若将重排器配置为 Identity，则相当于跳过重排阶段，适用于算力有限或只需基础流程的场景。

### RRF Hybrid

RRF（Reciprocal Rank Fusion）通过为每个候选在各个排序列表中的位置赋予倒数分数来计算综合得分。排名越靠前，倒数分数越高。

RRF 会对候选在所有初始列表中的倒数分数求和，并根据总分重新排序；那些在多个列表中表现优异的项会自动靠前，实现多种检索方法融合后的稳健排序。

## 如何配置使用的重排器？

在 MemMachine 的配置文件中添加相应条目即可选择所需重排器，例如：

```bash
# 在 cfg.yml 中添加相应重排器配置
# 具体示例请参阅配置文档
```



